.. -*- mode: rst -*-
.. include:: ../definitions.rst

.. TODO: update cspy reference to more recent book
.. TODO: add some literature references (esp to other intro linguistics textbooks)
.. TODO: adopt simpler hacker example with only single character transpositions;
   move hacker example to later section (later chapter?)
.. TODO: get URL hyperlinks to be fixed width
.. TODO: websites with automatically generated language -- lobner prize... 

.. _chap-introduction:

===========================
1. Обработка языка и Python
===========================

Нам доступны миллионы слов текста.
Как мы можем обрабатывать их при помощи простых программ?
В этой главе мы рассмотрим следующие вопросы.

#. Какие задачи можно решать, применяя простые методы программирования к большим объемам текста?
#. Как автоматически извлекать ключевые слова и фразы, определяющие стиль и содержание текста?
#. Какие инструменты и методы для решения таких задач предоставляет язык Python?
#. Какие интересные задачи можно решать в рамках обработки естественного языка?

Эта глава состоит из разделов двух видов, которые заметно отличаются друг от друга.
В разделах "вычисления над языком" мы будем рассматривать некоторые связанные с
лингвистикой задачи программирования без подробного объяснения механизма работы программ.
А в разделах "подробнее о Python" мы будем системно изучать основные принципы и
понятия программирования. Вид раздела будет указываться в его заголовке; однако в
последующих главах мы будем сочетать оба подхода без строгого отнесения разделов
к той или иной категории. Мы надеемся, что стиль введения поможет вам почувствовать,
о чем пойдет речь в дальнейшем; при этом мы рассмотрим большое число элементарных понятий
из области лингвистики и программирования. Если вы обладаете базовыми знаниями
в этих областях, вы можете сразу переходить к разделу
sec-automatic-natural-language-understanding_;
мы вернемся к наиболее важным идеям в последующих главах, а если что-либо останется для вас непонятным, вы сможете
воспользоваться справочными материалами, размещенными в Интернете по адресу |NLTK-URL|.
Если вы впервые встречаетесь с данной тематикой, эта глава вызовет больше вопросов,
чем даст ответов; ответы на эти вопросы вы найдете
в других главах данной книги.

.. _sec-computing-with-language-texts-and-words:

-------------------------------------
Вычисления над языком: тексты и слова
-------------------------------------

Мы хорошо знаем, что такое текст, поскольку мы читаем и пишем каждый день. В этой книге мы будем воспринимать
текст как `сырые данные`:em: для программ, которые мы пишем
и которые обрабатывают этот текст и выполняют с ним различные интересные операции. Но прежде чем мы
сможем написать нашу первую программу, мы должны научиться работать с интерпретатором Python.

Начало работы с Python
----------------------

Одна из привлекательных особенностей языка Python состоит в том, что команды
можно вводить непосредственно в интерактивный `интерпретатор`:dt: |mdash|
программу, которая будет выполнять наши программы на языке Python. С интерпретатором Python
можно работать с помощью простого графического интерфейса, который
называется (|IDLE|).  (Interactive Development Environment, интерактивная среда разработки).
На компьютерах Mac она запускается из меню *Приложения*\ |rarr|\ *MacPython*,
а в Windows |mdash| из раздела *Все программы*\ |rarr|\ *Python*.
Чтобы запустить Python в Unix, достаточно ввести в командной оболочке ``idle``
(если интерактивная среда не установлена, попробуйте ввести ``python``). Интерпретатор
выведет сведения об установленной версии Python; просто проверьте, что
установлена версия Python 2.4 или 2.5 (в данной книге используется версия 2.5.1).

.. doctest-ignore::
    Python 2.5.1 (r251:54863, Apr 15 2008, 22:57:26) 
    [GCC 4.0.1 (Apple Inc. build 5465)] on darwin
    Type "help", "copyright", "credits" or "license" for more information.
    >>>

.. note::
   Если запустить интерпретатор Python не удается, возможно, Python
   установлен неправильно. Подробные инструкции
   см. на сайте |PYTHON-URL|.

Символы ``>>>`` (приглашение) указывают, что интерпретатор Python ожидает ввода команд.
При копировании примеров из книги не копируйте символы "``>>>``".
Для начала попробуем использовать Python в качестве калькулятора:

    >>> 1 + 5 * 2 - 3
    8
    >>>

Когда интерпретатор завершит вычисления и выведет ответ, снова появится приглашение.
Это означает, что интерпретатор Python ожидает новой инструкции.

.. note:: |TRY|
   Придумайте и введите еще несколько выражений. Вы можете
   использовать звездочку (``*``) в качестве знака умножения, косую черту (``/``)
   в качестве знака деления и скобки для группировки операций. Обратите внимание,
   что результат деления может отличаться от ожидаемого |mdash| если ввести ``1/3``,
   будет выполнено целочисленное деление (с округлением до ближайшего целого в меньшую сторону),
   а если ввести ``1.0/3.0``, будет выполнено деление "с плавающей запятой" (или десятичное деление).
   Чтобы добиться деление всегда выполнялось привычным образом
   (в Python 3.0 это является стандартным поведением), введите команду ``from __future__ import division``.

.. XXX The following example currently wraps over a page boundary, which
   makes it difficult to read, esp since you can't see where the "^" is
   pointing.

Мы увидели, как работать с интерпретатором Python
в интерактивном режиме, экспериментируя с различными выражениями. Попробуем
теперь ввести выражение, не имеющее смысла, и посмотрим,
как интерпретатор обработает его:

    >>> 1 +
      File "<stdin>", line 1 
        1 +
          ^
    SyntaxError: invalid syntax
    >>>

Появилась `"синтаксическая ошибка"`:dt:. В Python инструкция,
оканчивающаяся на знак "плюс", не имеет смысла. Кроме того, интерпретатор Python указывает строку,
где возникла ошибка (строка 1 в ``<stdin>``,
что означает "standard input" (стандартный ввод)).

Итак, мы научились работать с интерпретатором Python, и можем
переходить к языковым данным.

Начало работы с NLTK
-------------------------

Перед тем как начать работу, необходимо установить пакет |NLTK|\, который можно загрузить по адресу |NLTK-URL|.
Выполните приведенные на этом сайте инструкции по установке версии, предназначенной для нужной платформы.

После установки пакета |NLTK| запустите интерпретатор Python, как было описано выше, и установите необходимые для
работы с этой книгой данные, введя в командной строке Python следующие два команды,
а затем выберите коллекцию ``book``, как показано на fig-nltk-downloader_.
   
.. doctest-ignore::
    >>> import nltk
    >>> nltk.download()
   
.. _fig-nltk-downloader:
.. figure:: ../images/nltk-downloader.png
   :scale: 100:100:100

   Загрузка коллекции книг NLTK: просмотрите доступные пакеты
   с помощью команды ``nltk.download()``. На вкладке **Collections** средства загрузки
   показано, что пакеты объединены в наборы, и для получения
   всех рассматриваемых в этой книге примеров и упражнений
   необходимо выбрать строку **book**. Этот набор
   состоит примерно из 30 сжатых файлов, занимающих на диске порядка 100 Мбайт.
   Полный массив данных (пункт **all** в средстве загрузки)
   имеет примерно в пять раз больший объем (на момент 
   написания книги) и продолжает расти.

После копирования данных на компьютер часть из них можно загрузить
для работы с помощью интерпретатора Python. Сначала необходимо ввести в
командную строку Python специальную команду, сообщающую интерпретатору,
что необходимо загрузить некоторые тексты
для изучения: ``from nltk.book import *``.
Это означает "из модуля ``book`` NLTK загрузить все элементы".
Модуль ``book`` содержит все данные, которые потребуются при чтении
этой главы. После вывода приветственного сообщения будут загружены
тексты нескольких книг (это займет несколько секунд). Ниже показана
сама команда и текст, выводимый на экран при ее выполнении. Следите
за правильностью написания команд и знаков препинания;
не забывайте, что вводить ``>>>`` не нужно.

    >>> from nltk.book import *
    *** Introductory Examples for the NLTK Book ***
    Loading text1, ..., text9 and sent1, ..., sent9
    Type the name of the text or sentence to view it.
    Type: 'texts()' or 'sents()' to list the materials.
    text1: Moby Dick by Herman Melville 1851
    text2: Sense and Sensibility by Jane Austen 1811
    text3: The Book of Genesis
    text4: Inaugural Address Corpus
    text5: Chat Corpus
    text6: Monty Python and the Holy Grail
    text7: Wall Street Journal
    text8: Personals Corpus
    text9: The Man Who Was Thursday by G . K . Chesterton 1908
    >>>

Чтобы получить информацию о том или ином тексте, достаточно ввести
имя этого текста в командную строку Python:

    >>> text1
    <Text: Moby Dick by Herman Melville 1851>
    >>> text2
    <Text: Sense and Sensibility by Jane Austen 1811>
    >>>

Итак, мы научились использовать интерпретатор Python, у нас есть данные для обработки,
и мы можем переходить к решению реальных задач.

Поиск по тексту
--------------

Есть множество способов определения контекста фрагмента текста,
не считая простого чтения этого фрагмента. Представление конкорданции
позволяет увидеть каждое вхождение заданного слова и фрагмент контекста.
Ниже мы будем искать слово ``"monstrous"``:lx: в книге *Moby Dick* ("Моби Дик"). Для этого мы введем ``text1``,
точку, ключевое слово ``concordance`` и слово ``"monstrous"`` в скобках.

    >>> text1.concordance("monstrous")
    Building index...
    Displaying 11 of 11 matches:
    ong the former , one was of a most monstrous size . ... This came towards us ,
    ON OF THE PSALMS . " Touching that monstrous bulk of the whale or ork we have r
    ll over with a heathenish array of monstrous clubs and spears . Some were thick
    d as you gazed , and wondered what monstrous cannibal and savage could ever hav
    that has survived the flood ; most monstrous and most mountainous ! That Himmal
    they might scout at Moby Dick as a monstrous fable , or still worse and more de
    th of Radney .'" CHAPTER 55 Of the monstrous Pictures of Whales . I shall ere l
    ing Scenes . In connexion with the monstrous pictures of whales , I am strongly
    ere to enter upon those still more monstrous stories of them which are to be fo
    ght have been rummaged out of this monstrous cabinet there is no telling . But
    of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u
    >>>

.. note:: |TRY|
   Попробуйте поискать другие слова. Чтобы не вводить одни и те же символы заново, можно
   использовать клавиши СТРЕЛКА ВВЕРХ, CTRL+СТРЕЛКА ВВЕРХ и ALT+P для перехода к предыдущей команде и изменения только искомого слова.
   Кроме того, вы можете выполнять поиск в других загруженных текстах. 
   Например, попробуйте найти слово `affection`:lx:
   в романе *Sense and Sensibility* ("Чувство и чувствительность"), задав команду ``text2.concordance("affection")``.  
   Из книги Genesis (Книга Бытия) попробуйте узнать, сколько жили
   те или иные люди, выполнив команду
   ``text3.concordance("lived")``. В тексте ``text4``
   *Inaugural Address Corpus* (Корпус инаугурационных речей) можно найти примеры английского языка c
   1789 года; в нем можно поискать такие слова, как `nation`:lx: (нация), `terror`:lx: (террор), `god`:lx: (бог),
   чтобы отследить особенности употребления этих слов в разное время.
   Мы также включили в набор корпус ``text5`` *NPS Chat Corpus* (Корпус сообщений в интернет-чатах, 
   подготовленный Школой повышения квалификации ВМС США): в нем можно поискать
   разговорные слова, такие как `im`:lx:, `ur`:lx:, `lol`:lx:.
   (Следует иметь в виду, что этот корпус не подвергался цензурной обработке!)

Мы надеемся, что после некоторого изучения этих текстов у вас появится
новое ощущение богатства и разнообразия языка. В следующей главе вы научитесь
работать с более широким массивом текстов, включая тексты на языках,
отличных от английского.

Функция конкорданции позволяет видеть слова в контексте. Например, мы узнали, что
слово `monstrous`:lx: ("чудовищный") встречалось контекстах `the ___ pictures`:lx: ("чудовищные изображения")
и `the ___ size`:lx: ("чудовищный размер"). Какие еще слова встречаются
в похожих контекстах?  Чтобы узнать это, добавим
ключевое слово ``similar`` к имени текста 
в запросе, а затем в скобках укажем соответствующее слово:

    >>> text1.similar("monstrous")
    Building word-context index...
    subtly impalpable pitiable curious imperial perilous trustworthy
    abundant untoward singular lamentable few maddens horrible loving lazy
    mystifying christian exasperate puzzled
    >>> text2.similar("monstrous")
    Building word-context index...
    very exceedingly so heartily a great good amazingly as sweet
    remarkably extremely vast
    >>>

Обратите внимание, что для разных текстов результаты различаются.  
Джейн Остин использует это слово не так, как Герман Мелвилл; у нее слово `monstrous`:lx: имеет
положительные коннотации и иногда используется для усиления впечатления, как, например, 
слово `very`:lx: (очень).  

Ключевое слово ``common_contexts`` позволяет рассматривать только контексты,
являющиеся общими для двух или более слов, например для слов `monstrous`:lx:
и `very`:lx:. Для этого необходимо поместить эти слова в квадратные
и круглые скобки и разделить запятой:

    >>> text2.common_contexts(["monstrous", "very"])
    be_glad am_glad a_pretty is_pretty a_lucky
    >>>

.. note:: |TRY|
   Выберите другую пару слов и попробуйте сравнить случаи их использования в двух различных текстах
   с помощью функций ``similar()`` и ``common_contexts()``.

Автоматический поиск вхождений определенного слова в тексте
и отображение слов, которые встречаются в том же контекста, |mdash| это лишь одна из доступных возможностей. Мы также можем определять
*положение* слова в тексте: на каком расстоянии от начала текста (в словах) оно находится.
Такую информацию о положении можно представить в виде "диаграммы рассеяния":dt:.
Каждый штрих соответствует одному вхождению
слова, а каждая строка представляет текст целиком.  На рисунке fig-inaugural_ мы
видим характерное распределение использования заданных слов на протяжении последних 220 лет
(в искусственном тексте, полученном путем последовательного объединения
текстов корпуса инаугурационных речей). 
Ниже показано, как построить такую диаграмму.
К ней можно добавить и другие слова (например, "liberty":lx: (свобода) или "constitution":lx: (конституция)),
а также другие тексты. Можете ли вы спрогнозировать
распределение для слова, прежде чем увидите диаграмму? Как и прежде, внимательно
следите за тем, чтобы правильно расставлять кавычки, запятые, круглые и фигурные скобки.

.. doctest-ignore::
    >>> text4.dispersion_plot(["citizens", "democracy", "freedom", "duties", "America"])
    >>>

.. _fig-inaugural:
.. figure:: ../images/inaugural.png
   :scale: 90:90:90

   Лексическая диаграмма рассеяния для слов в инаугурационных речах президентов США.
   Она позволяет исследовать изменения, происходящие в языке со временем.

.. note:: |IMPORTANT|
   Чтобы строить используемые в этой книге графические диаграммы, необходимо установить пакеты Python
   NumPy и Matplotlib.
   Инструкции по установке см. на сайте |NLTK-URL|.

Попробуем ради интереса сгенерировать случайным образом несколько текстов в
стилях, которые мы только что видели. Для этого нужно задать имя текста
и команду ``generate``. (Также следует добавить скобки,
но оставить их пустыми.)

    >>> text3.generate()
    In the beginning of his brother is a hairy man , whose top may reach
    unto heaven ; and ye shall sow the land of Egypt there was no bread in
    all that he was taken out of the month , upon the earth . So shall thy
    wages be ? And they made their father ; and Isaac was old , and kissed
    him : and Laban with his cattle in the midst of the hands of Esau thy
    first born , and Phichol the chief butler unto his son Isaac , she
    >>>

Обратите внимание, что первое выполнение этой команды занимает много времени, поскольку
программа собирает статистику использования последовательностей слов. При каждом запуске команды формируется новый текст.
Теперь попробуйте создать случайный текст в стиле инаугурационной речи или
интернет-чата. Хотя текст формируется случайным образом, в нем используются типичные слова
и фразы из исходного текста, что позволяет понять его стиль и содержание.
(Чего не хватает таким сгенерированным случайным образом текстам?)

.. note::
   При создании текстов с помощью команды ``generate`` знаки препинания отделяются
   от предшествующих им слов. Хотя такое форматирование и не соответствует
   правилам английского языка, мы сделали это намеренно, чтобы подчеркнуть, что
   слова и знаки препинания не зависят друг от друга. Этот вопрос
   будет подробнее рассмотрен в chap-words_.

Подсчет элементов словаря
-------------------------

Из рассмотренных выше примеров хорошо видно,
что тексты различаются используемым в них словарем. В этом разделе мы увидим, как с помощью
компьютера подсчитывать слова в тексте для решения различных практических задач.
Как и прежде, мы начнем с примера кода для
интерпретатора Python, хотя вы еще не изучили Python достаточно
глубоко. Проверьте себя, изменяя примеры или выполняя
упражнения, приведенные в конце главы.

Начнем с определения полной длины текста,
измеряемой в словах и знаках препинания. Для определения
длины чего-либо используется команда ``len``, которую мы применим к
Книге Бытия:

    >>> len(text3)
    44764
    >>>

Т. о. Книга Бытия состоит из 44764 слов и знаков препинания (или "токенов").
"Токен":dt: |mdash| это термин, обозначающий последовательность символов,
например ``hairy``, ``his`` или ``:)``, которая должна рассматриваться
как единое целое. При подсчете числа токенов в тексте, например во фразе
`to be or not to be`:lx:, мы подсчитываем число вхождений таких
последовательностей. Таким образом, в нашей фразе два вхождения `to`:lx:,
два вхождения `be`:lx: и по одному вхождению `or`:lx: и `not`:lx:. Но в этой
фразе есть лишь четыре различных элемента словаря.
Сколько различных слов содержится в Книге Бытия?
Чтобы определить это с помощью Python, нужно поставить вопрос несколько
иначе. Словарь текста - это *набор* используемых в тексте токенов;
для всех повторяющихся токенов в таком наборе остается только одно вхождение.
Чтобы получить элементы словаря для текста ``text3`` с помощью в Python,
необходимо выполнить команду ``set(text3)``. При этом на экране быстро промелькнут
длинные списки слов. Вместо этого попробуйте выполнить следующую команду: 

    >>> sorted(set(text3)) # [_sorted-set]
    ['!', "'", '(', ')', ',', ',)', '.', '.)', ':', ';', ';)', '?', '?)',
    'A', 'Abel', 'Abelmizraim', 'Abidah', 'Abide', 'Abimael', 'Abimelech',
    'Abr', 'Abrah', 'Abraham', 'Abram', 'Accad', 'Achbor', 'Adah', ...]
    >>> len(set(text3)) # [_len-set]
    2789
    >>>

Помещая выражение ``set(text3)`` внутрь команды Python ``sorted()``
sorted-set_,  мы получаем отсортированный список элементов словаря, в начале
которого находятся знаки препинания, за которыми следуют слова на `A`:lx: и т. д. Все
начинающиеся с прописной буквы слова стоят перед словами, которые начинаются со строчной буквы.
Мы определяем размер словаря не напрямую, а запрашиваем
число элементов в наборе, и для получения этого числа мы снова используем команду ``len``
len-set_. Хотя книга содержит 44764 токенов, различных слов (так называемых "типов слов")
в ней только 2789.
`Тип слова`:dt: |mdash| это форма или написание слова независимо от его
конкретных вхождений в текст; то есть словом
считается каждый уникальный элемент словаря. Полученная сумма 2789
также включает также знаки препинания, поэтому в общем случае эти
уникальные элементы называются просто `типами`:dt:, а не типами слов. 

Теперь определим меру лексического богатства
текста. В следующем примере показано, что каждое слово в среднем используется 16 раз
(следите за тем, чтобы интерпретатор Python использовал деление чисел с плавающей запятой):

    >>> from __future__ import division
    >>> len(text3) / len(set(text3))
    16.050197203298673
    >>>

Обратимся к конкретным словам. Мы можем подсчитать, как часто определенное
слово встречается в тексте, а также определить долю, занимаемую словом в тексте (в процентах):

    >>> text3.count("smote")
    5
    >>> 100 * text4.count('a') / len(text4)
    1.4643016433938312
    >>>

.. note:: |TRY|
   Сколько раз слово `lol`:lx: встречается в тексте ``text5``?
   Какой процент составляет это число от общего количества слов
   в данном тексте? 

Возможно, вам потребуется повторить эти расчеты для нескольких текстов,
но каждый раз заново вводить формулу неудобно. Вместо этого
можно обозначить данную процедуру определенным именем, например
"lexical_diversity" или "percentage", и связать ее с фрагментом кода.
После этого достаточно будет вводить лишь короткое имя,
а не одну или несколько строки кода Python,
чтобы повторять одну и ту же процедуру столько раз, сколько потребуется. Такой выполняющий определенную
задачу фрагмент кода называется `функцией`:dt:, а
для задания короткого имени функции используется ключевое слово ``def``.
В следующем примере показано, как определить две новых функции
``lexical_diversity()``и ``percentage()``:

    >>> def lexical_diversity(text): # [_fun-parameter1]
    ...     return len(text) / len(set(text)) # [_locvar]
    ...
    >>> def percentage(count, total): # [_fun-parameter2]
    ...     return 100 * count / total
    ...

.. Caution::
   Интерпретатор Python заменяет оформление командной строки с
   ``>>>`` на ``...`` при обнаружении двоеточия
   в конце первой строки. Символы ``...`` указывают,
   что интерпретатор Python ожидает, что далее будет располагаться `блок кода с отступом`:dt:.
   Пользователь сам должен создать этот отступ, введя четыре
   пробела или нажав клавишу TAB. Чтобы завершить блок с отсупом,
   добавьте пустую строку.

В определении функции ``lexical_diversity()`` fun-parameter1_ мы
задали `параметр`:dt: с именем ``text``. Вместо этого
параметра будет подставляться имя конкретного текста, степень разнородности
которого мы рассчитываем; имя этого параметра также встречается в коде, который будет выполняться
при вызове функции locvar_. Аналогично определяется функция ``percentage()``, которая
принимает два параметра с именами ``count`` и ``total`` fun-parameter2_.

Теперь интерпретатору Python "известно", что
``lexical_diversity()`` и ``percentage()`` являются именами определенных
фрагментов кода, и мы можем начать использование этих функций:

    >>> lexical_diversity(text3)
    16.050197203298673
    >>> lexical_diversity(text5)
    7.4200461589185629
    >>> percentage(4, 5)
    80.0
    >>> percentage(text4.count('a'), len(text4))
    1.4643016433938312
    >>> 

Итак, чтобы `вызвать`:dt: функцию, например функцию ``lexical_diversity()``, необходимо ввести ее имя,
открывающую скобку, имя текста и закрывающую
скобку. Такие скобки мы будут встречаться нам довольно часто; они отделяют
имя процедуры |mdash| например, ``lexical_diversity()`` |mdash| от данных, к
которым будет применена эта процедура |mdash| например, ``text3``.
Значение, которое мы помещаем в скобки при вызове функции,
называется `аргументом`:dt: этой функции.

В этой главе мы уже видели несколько функций, например 
``len()``, ``set()`` и ``sorted()``. Принято всегда
добавлять к имени функции пустую пару скобок, например
``len()``, чтобы было понятно, что речь идет именно о функции,
а не о каком-либо другом типе выражений Python.
Функции являются очень важным понятием в программировании, и мы
упоминаем их в самом начале, чтобы новички могли почувствовать
мощь и гибкость программирования. Не переживайте, если на данном этапе
все это кажется слишком сложным. 

Ниже мы увидим, как с помощью функций представлять данные в виде таблиц, как показано в tab-brown-types_.
Каждая строка таблицы строится на базе одних и тех же расчетов,
но с различными данными, и мы проделаем эти повторяющиеся операции с помощью функции.

.. table:: tab-brown-types

   ==================  ======  =====  =================
   Genre               Tokens  Types  Lexical diversity
   ==================  ======  =====  =================
   skill and hobbies   82345   11935  6.9
   humor               21695   5017   4.3
   fiction: science    14470   3233   4.5
   press: reportage    100554  14394  7.0
   fiction: romance    70022   8452   8.3
   religion            39399   6373   6.2
   ==================  ======  =====  =================

   Лексическое разнообразие различных жанров в *корпусе Брауновского университета*

.. _sec-a-closer-look-at-python-texts-as-lists-of-words:

-----------------------------------------
Подробнее о Python: текст как список слов
-----------------------------------------

.. reimport

    >>> from nltk.book import *
    >>> def lexical_diversity(text):
    ...     return len(text) / len(set(text))

Вы уже познакомились с некоторыми основными элементами языка программирования Python.
Давайте сделаем паузу, чтобы изучить их более подробно.

Списки
-----

.. XXX it's a little confusing that we assign a value to sent1 here,
   when it's already received on from the "from nltk.book import *"
   statement.  Granted it's the same value, but still...

Что такое текст?  С одной стороны это последовательность символов
на странице, подобная той, которую вы сейчас читаете. С другой |mdash| это последовательность глав, составленных
из последовательностей разделов, где каждый раздел представляет собой последовательность абзацев
и т. д. Однако мы рассматриваем текст, лишь как
последовательность слов и знаков препинания. Ниже показано представление
текста в языке Python; это первое предложение книги *Moby Dick*:

    >>> sent1 = ['Call', 'me', 'Ishmael', '.']
    >>>

После начальной строки стоит придуманное нами имя, ``sent1``, за
которым следуют знак равенства и последовательность заключенных в кавычки и
разделенных запятыми слов, помещенная в квадратные скобки. Такие данные в
квадратных скобках в языке Python называются `списком`:dt:; именно списки мы используем для хранения текстов.
Чтобы увидеть содержимое списка, введите его имя inspect-var_. Мы также можем узнать длину списка len-sent_.
Кроме того, мы можем применить к списку нашу функцию ``lexical_diversity()`` apply-function_.

    >>> sent1 # [_inspect-var]
    ['Call', 'me', 'Ishmael', '.']
    >>> len(sent1) # [_len-sent]
    4
    >>> lexical_diversity(sent1) # [_apply-function]
    1.0
    >>>

Еще несколько списков были заранее определены
для первых предложений каждого из текстов:
``sent2`` |dots| ``sent9``.  Здесь мы рассмотрим два
из них; вы можете изучить остальные самостоятельно, воспользовавшись интерпретатором Python
(если появится сообщение об ошибке "``sent2`` is not defined", необходимо
предварительно выполнить команду ``from nltk.book import *``).

    >>> sent2
    ['The', 'family', 'of', 'Dashwood', 'had', 'long',
    'been', 'settled', 'in', 'Sussex', '.']
    >>> sent3
    ['In', 'the', 'beginning', 'God', 'created', 'the',
    'heaven', 'and', 'the', 'earth', '.']
    >>>

.. note:: |TRY|
   Самостоятельно создайте несколько предложений; для этого введите имя,
   знак равенства и список слов, как показано ниже:
   ``ex1 = ['Monty', 'Python', 'and', 'the', 'Holy', 'Grail']``.
   Повторите некоторые их операций, которые мы рассматривали в разделе
   sec-computing-with-language-texts-and-words_,
   например ``sorted(ex1)``, ``len(set(ex1))``, ``ex1.count('the')``.

Приятный сюрприз заключается в том, что мы можем применять к спискам оператор сложения Python.
В результате сложения двух списков list-plus-list_ получается новый список, который
содержит все элементы первого списка, за которыми следуют все 
элементы второго списка:

    >>> ['Monty', 'Python'] + ['and', 'the', 'Holy', 'Grail'] # [_list-plus-list]
    ['Monty', 'Python', 'and', 'the', 'Holy', 'Grail']

.. note::
   Такое использование операции сложения называется `конкатенацией`:dt:;
   при этом несколько списков объединяются в один список. С помощью конкатенации можно объединять
   предложения, чтобы составить текст.

Вводить списки в явном виде тоже не обязательно; можно использовать
короткие имена, которые задают ранее созданные списки.

    >>> sent4 + sent1 
    ['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the',
    'House', 'of', 'Representatives', ':', 'Call', 'me', 'Ishmael', '.']
    >>>

Предположим, мы хотим добавить в список один элемент.
Для этого к списку необходимо применить функцию ``append()``, в результате чего
список изменится.

    >>> sent1.append("Some")
    >>> sent1
    ['Call', 'me', 'Ishmael', '.', 'Some']
    >>> 

Индексация списков
------------------

.. XXX I think a picture would be very helpful for this section, namely
   one showing something like:
       | Call   | me     | Ishmael | .     |
       0        1        2         3       4
   This might obviate the need to use a contrived sentence "word1 
   word2 etc".  I find this picture especially useful for understanding
   slicing, but it also gives a reasonable motivation for zero-indexing.

.. XXX If we end up doing this, can we offset the integers slightly so
   that they (just) fall inside the corresponding cell?

       +--------+--------+--------+--------+
       | Call   | me     | Ishmael| .      |
       |0       |1       |2       |3       |4
       +--------+--------+--------+--------+

Как мы уже видели, текст в языке Python |mdash| это список слов, представляемый
набором квадратных скобок и кавычек. Как и в случае обычной
страницы текста, мы можем подсчитать общее число слов в тексте ``text1``
с помощью команды ``len(text1)`` и количество вхождений в текст определенного
слова, например слова ``'heaven'``, с помощью команды ``text1.count('heaven')``.

Если приложить определенные усилия, можно найти 1-е, 173-е или даже 14278-е
слово в напечатанном тексте. Точно так же мы можем идентифицировать элементы
списка Python по их порядковому положению в этом списке. Число, представляющее
позицию элемента, называется `индексом`:dt: этого элемента. Чтобы с помощью Python
узнать, какой элемент находится в тексте на позиции с индексом ``173``,
необходимо указать имя текста и индекс в квадратных скобках:

    >>> text4[173]
    'awaken'
    >>>

Можно проделать и обратную операцию |mdash| для заданного слова найти индекс позиции, где
оно впервые встречается:

    >>> text4.index('awaken')
    173
    >>>

Индексы |mdash| это стандартный способ обращения к словам текста
или в общем случае к элементам любого списка.
Язык Python также позволяет работать с подсписками, извлекая
из крупных текстов необходимые фрагменты. Этот подход называется
`извлечением фрагмента`:dt:.

    >>> text5[16715:16735]
    ['U86', 'thats', 'why', 'something', 'like', 'gamefly', 'is', 'so', 'good',
    'because', 'you', 'can', 'actually', 'play', 'a', 'full', 'game', 'without',
    'buying', 'it']
    >>> text6[1600:1625]
    ['We', "'", 're', 'an', 'anarcho', '-', 'syndicalist', 'commune', '.', 'We',
    'take', 'it', 'in', 'turns', 'to', 'act', 'as', 'a', 'sort', 'of', 'executive',
    'officer', 'for', 'the', 'week']
    >>>

В работе с индексами есть определенные тонкости, которые мы рассмотрим
на примере искусственного предложения:

    >>> sent = ['word1', 'word2', 'word3', 'word4', 'word5',
    ...         'word6', 'word7', 'word8', 'word9', 'word10']
    >>> sent[0]
    'word1'
    >>> sent[9]
    'word10'
    >>>

Обратите внимание, что отсчет индексов начинается с нуля: нулевым элементом списка ``sent`` (обозначается как``sent[0]``)
является первое слово (``'word1'``), а девятым элементом ``sent`` является слово ``'word10'``. 
Это объясняется очень просто: в момент, когда программа Python обращается к содержимому списка
в памяти компьютера, она уже находится на первом элементе;
и мы должны указать ей, на сколько элементов вперед необходимо продвинуться.
Таким образом, чтобы остаться на первом элементе, нужно сделать ноль шагов вперед.

.. note::
   Отсчет индексов от нуля может на начальном этапе сбивать с толку,
   но это стандартный подход для современных языков программирования.
   Вы быстро освоите его, если вы разобрались с системой
   подсчета веков, где год 19XY относится
   к 20-му веку, либо если вы живете в стране, где этажи
   в домах нумеруются от 1, и поэтому при подъеме по лестнице на `n-1`:math: этажей
   вы попадете на этаж `n`:math:. 

Если случайно задать слишком большое значение индекса, появится сообщение об ошибке:

    >>> sent[10]
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    IndexError: list index out of range
    >>>

В данном случае это не синтаксическая ошибка, поскольку фрагмент программы является синтаксически правильным.
Это так называемая ошибка `времени выполнения`:dt:, выдающая сообщение ``Traceback`` (обратная трассировка), в котором
содержатся контекст ошибки, ее имя
``IndexError`` и краткое описание.

На примере нашего искусственного предложения подробнее рассмотрим операцию извлечения фрагмента.
В следующем фрагменте кода мы проверяем, что отрезок ``5:8`` включает элементы ``sent``
с индексами 5, 6, и 7:

    >>> sent[5:8]
    ['word6', 'word7', 'word8']
    >>> sent[5]
    'word6'
    >>> sent[6]
    'word7'
    >>> sent[7]
    'word8'
    >>>

Обозначение ``m:n`` задает элементы `m`:mathit:\ |dots|\ `n-1`:mathit:.
Как показано в следующем примере,
первое число можно опустить, если фрагмент начинается с начала
списка list slice2_; а если фрагмент доходит до конца списка, можно опустить второе число slice3_:

    >>> sent[:3] # [_slice2]
    ['word1', 'word2', 'word3']
    >>> text2[141525:] # [_slice3]
    ['among', 'the', 'merits', 'and', 'the', 'happiness', 'of', 'Elinor', 'and', 'Marianne',
    ',', 'let', 'it', 'not', 'be', 'ranked', 'as', 'the', 'least', 'considerable', ',',
    'that', 'though', 'sisters', ',', 'and', 'living', 'almost', 'within', 'sight', 'of',
    'each', 'other', ',', 'they', 'could', 'live', 'without', 'disagreement', 'between',
    'themselves', ',', 'or', 'producing', 'coolness', 'between', 'their', 'husbands', '.',
    'THE', 'END']
    >>>

Задаваемые индексами элементы списка можно изменять, присваивая им значения.
В следующем примере ``sent[0]`` находится слева от знака равенства list-assignment_.  Мы также можем
заменить целый фрагмент slice-assignment_.  В результате последнего
изменения список сократится до четырех элементов, и обращение к элементу с большим индексом
приведет к ошибке list-error_.

    >>> sent[0] = 'First' # [_list-assignment]
    >>> sent[9] = 'Last'
    >>> len(sent)
    10
    >>> sent[1:9] = ['Second', 'Third'] # [_slice-assignment]
    >>> sent
    ['First', 'Second', 'Third', 'Last']
    >>> sent[9] # [_list-error]
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    IndexError: list index out of range
    >>>    

.. note:: |TRY|
   Потратьте немного времени на то, чтобы определить собственное предложение и изменить в нем отдельные
   слова или группы слов (фрагменты) с использованием вышеописанных подходов. Проверьте себя
   с помощью приведенных в конце главы упражнений, посвященных спискам.

Переменные
---------

С самого начала раздела sec-computing-with-language-texts-and-words_ мы обращались
к текстам с именами ``text1``, ``text2`` и т. д.  Такой подход позволял нам
не тратить время на набор больших объемов текста и обращаться к книге, состоящей из 250 тысяч слов,
с помощью короткого имени. В общем случае мы можем определить имена для всего, что нам
требуется при расчетах. Мы уже делали это ранее, например когда
определяли `переменную`:dt: ``sent1`` следующим образом:

    >>> sent1 = ['Call', 'me', 'Ishmael', '.']
    >>>

Эти строки имеют формат *переменная = выражение*. Python вычисляет
выражения и сохраняет результат в переменной. Такая операция
называется `присваиванием`:dt:. Она не формирует вывода;
чтобы проверить содержимое переменной, необходимо
ввести имя этой переменной на отдельной строке. Знак равенства может немного сбивать с толку,
поскольку информация перемещается справа налево.
Возможно, вместо него лучше представлять себе направленную влево стрелку.
Имя переменной можно выбирать произвольным образом, например ``my_sent``, ``sentence``, ``xyzzy``.
Оно должно начинаться с буквы и может включать цифры и знаки подчеркивания.
Вот несколько примеров переменных и присваиваний:
  
    >>> my_sent = ['Bravely', 'bold', 'Sir', 'Robin', ',', 'rode',
    ... 'forth', 'from', 'Camelot', '.']
    >>> noun_phrase = my_sent[1:4]
    >>> noun_phrase
    ['bold', 'Sir', 'Robin']
    >>> wOrDs = sorted(noun_phrase)
    >>> wOrDs
    ['Robin', 'Sir', 'bold']
    >>>

Помните, что в отсортированных списках слова, начинающиеся с большой буквы, находятся выше.

.. note::
   В предыдущем примере обратите внимание на то, что определение списка
   ``my_sent`` занимает две строки. Выражения Python можно делить на
   несколько строк, если разрыв строки находится внутри каких-либо скобок.
   Для обозначения командных строк, предполагающих дальнейший ввод кода, используются
   символы "``...``". Шаг отступа в таких строках значения не имеет,
   однако даже небольшой отступ делает их более удобочитаемыми.

Рекомендуется подбирать осмысленные имена переменных, чтобы вам или другим пользователям,
читающим ваш код Python, было понятно, для чего предназначен этот код.
Интерпретатор Python не пытается "понять" смысл имен, а лишь вслепую выполняет инструкции,
поэтому даже операции ``one = 'two'`` и ``two = 3`` являются формально корректными.
Единственное ограничение связано с тем,
что имя переменной не может совпадать с каким-либо зарезервированным словом языка Python, например
``def``, ``if``, ``not``
и ``import``.  Если вы воспользуетесь зарезервированным словом, интерпретатор Python вернет синтаксическую ошибку:

    >>> not = 'Camelot'
    File "<stdin>", line 1
        not = 'Camelot'
            ^
    SyntaxError: invalid syntax
    >>>

Переменные часто используются для хранения промежуточных результатов вычислений, особенно
если это упрощает чтение и понимание кода. Так, например, выражение ``len(set(text1))`` можно переписать следующим образом:

    >>> vocab = set(text1)
    >>> vocab_size = len(vocab)
    >>> vocab_size
    19317
    >>>

.. Caution::
   Внимательно подходите к выбору имен (также называемых `идентификаторами`:dt:) переменных Python.
   Во-первых, имя должно начинаться с буквы, за которой могут следовать буквы или цифры(от ``0`` до ``9``). Поэтому ``abc23`` является допустимым
   именем, а имя ``23abc`` приведет к синтаксической ошибке. 
   Имена задаются с учетом регистра, поэтому ``myVar`` и ``myvar``
   являются различными переменными. Имена переменных не могут содержать пробелов, но
   для разделения слов можно использовать подчеркивание, например
   ``my_var``. Следите за тем, чтобы вместо подчеркивания случайно не
   вставить дефис: ``my-var`` |mdash| неправильное имя, поскольку Python воспринимает
   "``-``" как знак минуса. 

Строки
-------

Некоторые механизмы, которые мы использовали для обращения к элементам списка, также можно применять к отдельным словам
(или `строкам`:dt:). Например, можно присвоить строку переменной assign-string_,
проиндексировать строку index-string_ и извлечь из строки фрагмент slice-string_:

    >>> name = 'Monty' # [_assign-string]
    >>> name[0] # [_index-string]
    'M'
    >>> name[:4] # [_slice-string]
    'Mont'
    >>>

Кроме того, к строкам можно применять операции умножения и сложения:

    >>> name * 2
    'MontyMonty'
    >>> name + '!'
    'Monty!'
    >>>

Мы можем объединить слова в списке в одну строку или разделить строку на элементы списка, как показано ниже:

    >>> ' '.join(['Monty', 'Python'])
    'Monty Python'
    >>> 'Monty Python'.split()
    ['Monty', 'Python']
    >>>

Мы вернемся к строкам в chap-words_.
На данный момент мы знаем о двух важных элементах 
|mdash| списках и строках |mdash|
и готовы вернуться к анализу текстов.

.. _sec-computing-with-language-simple-statistics:

-----------------------------------------
Вычисления над языком: простая статистика
-----------------------------------------

.. reimport

   >>> from nltk.book import *

Вернемся к изучению способов применения вычислительных ресурсов
к обработке больших объемов текста. Мы начали рассматривать эти вопросы в
sec-computing-with-language-texts-and-words_ и увидели, как искать слова
в контексте, как составлять словарь текста, как генерировать случайный
в стиле имеющегося текста и т. д.

В этом разделе мы посмотрим, что делает текст отличным от других,
и научимся использовать автоматические методы для поиска в тексте характерных слов и выражений.
Как и в sec-computing-with-language-texts-and-words_ вы можете проверять
на практике изучаемые возможности языка Python, копируя код в интерпретатор;
а в следующем разделе мы расскажем об этих возможностях более подробно.

Прежде чем продолжить, предлагаем вам проверить, насколько хорошо вы усвоили
предыдущую главу, и предсказать результат выполнения следующего кода. Воспользуйтесь
интерпретатором, чтобы узнать, правильно ли вы ответили на вопрос. Если вы не знаете,
как выполнить это задание, рекомендуем еще раз изучить предыдущий раздел,
перед тем как двигаться дальше.

.. doctest-ignore::
    >>> saying = ['After', 'all', 'is', 'said', 'and', 'done',
    ...           'more', 'is', 'said', 'than', 'done']
    >>> tokens = set(saying)
    >>> tokens = sorted(tokens)
    >>> tokens[-2:]
    what output do you expect here?
    >>>



Частотные распределения
-----------------------

Как автоматически выделить в тексте слова, которые лучше всего
характеризуют предмет и жанр текста? Представим, что нам нужно
найти 50 чаще всего использующихся слов книги. Один из способов |mdash|
подсчитывать каждый элемент словаря, как показано на fig-tally_.
При этом полученная таблица будет состоять из тысяч строк, а сама задача окажется
настолько трудоемкой, что будет лучше поручть ее компьютеру.

.. _fig-tally:
.. figure:: ../images/tally.png
   :scale: 20:100:25

   Подсчет встречающихся в тексте слов (частотное распределение)

Таблица на fig-tally_ также называется `частотным распределением`:dt:,
и показывает частоту, с которой каждый элемент словаря встречается в тексте.
(В общем случае таким образом можно подсчитывать любые наблюдаемые события.)
Это "распределение", поскольку оно показывает нам, как общее количество токенов в тексте
распределено между отдельными элементами словаря.
Поскольку при обработке текстов частотные распределения бывают нужны достаточно часто, они
поддерживаются в пакете NLTK. Воспользуемся функцией ``FreqDist``, чтобы найти
50 чаще всего используемых слов в книге *Moby Dick*. Попробуйте самостоятельно понять, как работает следующий код,
а затем прочтите приведенное ниже объяснение.

    >>> fdist1 = FreqDist(text1) # [_freq-dist-call]
    >>> fdist1 # [_freq-dist-inspect]
    <FreqDist with 260819 outcomes>
    >>> vocabulary1 = fdist1.keys() # [_freq-dist-keys]
    >>> vocabulary1[:50] # [_freq-dist-slice]
    [',', 'the', '.', 'of', 'and', 'a', 'to', ';', 'in', 'that', "'", '-',
    'his', 'it', 'I', 's', 'is', 'he', 'with', 'was', 'as', '"', 'all', 'for',
    'this', '!', 'at', 'by', 'but', 'not', '--', 'him', 'from', 'be', 'on',
    'so', 'whale', 'one', 'you', 'had', 'have', 'there', 'But', 'or', 'were',
    'now', 'which', '?', 'me', 'like']
    >>> fdist1['whale']
    906
    >>>

Сначала мы вызываем функцию ``FreqDist`` и передаем ей имя текста
в качестве аргумента freq-dist-call_. Мы можем видеть общее число слов ("outcomes"),
полученное при подсчете freq-dist-inspect_ |mdash| 260 819 для
книги *Moby Dick*. Выражение ``keys()`` возвращает список всех
различающихся типов в тексте freq-dist-keys_, и чтобы увидеть
50 самых распространенных из них, мы извлекаем из списка его фрагмент freq-dist-slice_.

.. note:: |TRY|
   Попробуйте самостоятельно повторить предыдущий пример построения частотного распределения для
   ``text2``. Внимательно следите за использованием скобок и прописных букв.
   При появлении сообщения об ошибке ``NameError: name 'FreqDist' is not defined``
   необходимо сначала выполнить команду ``from nltk.book import *``

.. SB: no period after the above import statement

Помогают ли какие-либо слова в последнем примере понять жанр или предмет текста?
Только одно слово `whale`:lx: (кит) является хотя бы немного информативным! Оно встречается более 900 раз.
Остальные слова ничего не говорят нам о тексте и представляют собой лишь "смазку" английского языка.
Какая доля текста приходится на эти слова?
Мы можем построить накопительный график частоты для этих слов,
воспользовавшись функцией ``fdist1.plot(50, cumulative=True)`` (fig-fdist-moby_).
Эти 50 слов составляют примерно половину книги!

.. _fig-fdist-moby:
.. figure:: ../images/fdist-moby.png
   :scale: 20:25:25

   Накопительный график частоты для 50 чаще всего встречающихся слов в книге *Moby Dick*:
   они составляют примерно половину токенов.

Если частые слова не помогают нам ответить на вопрос, попробуем воспользоваться словами, которые
встречаются только по одному разу, или так называемыми `гапаксами`:dt:. Чтобы увидеть их, введите ``fdist1.hapaxes()``.
Этот список содержит слова `lexicographer`:lx:, `cetological`:lx:,
`contraband`:lx:, `expostulations`:lx: и еще примерно 9000 слов.
Похоже, что редких слов очень много, и, рассматривая их вне
контекста, мы вряд ли сможем понять значение даже половины из них!
Поскольку ни частые, ни редкие слова не дают нам ответа на вопрос, мы должны попробовать
найти другое решение.

"Тонкий" подобр слов
--------------------

Рассмотрим *длинные* слова текста; возможно, они окажутся
более информативными. Для этого мы воспользуемся обозначениями, принятыми
в теории множеств. Нам нужно найти в словаре слова,
длина которых превышает 15 букв. Введем
свойство `P`:math: такое, что `P(w)`:math: является истинным
тогда и только тогда, когда `w`:math: длиннее 15 букв.
Теперь интересующее нас подмножество слов можно описать математически, 
как показано ниже ex-set-comprehension-math.
Здесь написано следующее: "множество всех `w`:math:, таких что `w`:math: является
элементом `V`:math: (словаря) и `w`:math: обладает свойством `P`:math:".

.. _ex-set-comprehension:
.. ex::
   .. _ex-set-comprehension-math:
   .. ex:: {`w`:math: | `w`:math: |element| `V`:math: & `P(w)`:math:\ }
   .. _ex-set-comprehension-python:
   .. ex:: ``[w for w in V if p(w)]``

Соответствующее выражение на языке Python приведено ниже ex-set-comprehension-python_.
(Следует иметь в виду, что результатом этого выражения является множество, а не список; т. е. это множество может содержать повторы.)
Обратите внимание, насколько похожи эти две формы записи. Сделаем еще один шаг и напишем
исполняемый код Python:

    >>> V = set(text1)
    >>> long_words = [w for w in V if len(w) > 15]
    >>> sorted(long_words)
    ['CIRCUMNAVIGATION', 'Physiognomically', 'apprehensiveness', 'cannibalistically',
    'characteristically', 'circumnavigating', 'circumnavigation', 'circumnavigations',
    'comprehensiveness', 'hermaphroditical', 'indiscriminately', 'indispensableness',
    'irresistibleness', 'physiognomically', 'preternaturalness', 'responsibilities',
    'simultaneousness', 'subterraneousness', 'supernaturalness', 'superstitiousness',
    'uncomfortableness', 'uncompromisedness', 'undiscriminating', 'uninterpenetratingly']
    >>>

Для каждого слова ``w`` в словаре ``V`` мы проверяем, превышает ли
``len(w)`` значение 15; все остальные слова
пропускаются. Позже мы более подробно рассмотрим этот синтаксис.

.. note:: |TRY|
   Попробуйте выполнить приведенный код в интерпретаторе Python,
   экспериментируя с различными текстами и пороговой длиной.
   Влияет ли на результат изменение имен переменных,
   например использование выражения ``[word for word in vocab if ...]``? 

Вернемся к поиску слов, которые лучше всего характеризуют текст.
Обратите внимание, что длинные слова в ``text4`` связаны с государством
|mdash| `constitutionally`:lx:, `transcontinental`:lx: |mdash|,
а длинные слова в ``text5`` подчеркивают неформальный характер текста:
`boooooooooooglyyyyyy`:lx: и `yuuuuuuuuuuuummmmmmmmmmmm`:lx:.
Нам удалось автоматически выявить слова, которые позволяют типизировать
текст? Похоже, что эти длинные слова чаще всего одновременно являются и редкими (т. е. гапаксами)
словами, поэтому есть смысл найти *часто встречающиеся*
длинные слова. Такой подход выглядит многообещающим, поскольку он позволит
отсечь часто встречающиеся короткие слова (например, артикль `the`:lx:) и редкие длинные слова
(например, `antiphilosophists`:lx:).
Ниже показаны все слова из корпуса чата,
длиннее семи букв и встречающиеся больше семи раз:

    >>> fdist5 = FreqDist(text5)
    >>> sorted([w for w in set(text5) if len(w) > 7 and fdist5[w] > 7])
    ['#14-19teens', '#talkcity_adults', '((((((((((', '........', 'Question',
    'actually', 'anything', 'computer', 'cute.-ass', 'everyone', 'football',
    'innocent', 'listening', 'remember', 'seriously', 'something', 'together',
    'tomorrow', 'watching']
    >>>

Обратите внимание на задание двух условий: ``len(w) > 7`` отбирает слова,
которые длиннее семи букв, а ``fdist5[w] > 7`` |mdash| слова, которые
встречаются чаще семи раз. Наконец-то нам удалось автоматически
выявить часто встречающиеся слова, передающие смысл
текста. Это скромный, но важный промежуточный итог: мы написали небольшой фрагмент кода,
который обрабатывает тысячи слов и выдает полезный результат.

Коллокации и биграммы
---------------------

`Коллокация`:dt: |mdash| это последовательность идущих подряд слов,
как правило, часто встречающаяся. Таким образом, `red wine`:lx: (красное вино) |mdash| является коллокацией, а `the
wine`:lx: |mdash| нет. Особенностью коллокации является ее устойчивость
к замене слов с похожим смыслом;
например, словосочетание `maroon wine`:lx: (темно-красное вино) выглядит очень странно и коллокацией не является.  

Чтобы работать с коллокациями, необходимо сначала извлечь
из текста список пар слов, также называемых `биграммами`:dt:. Это легко сделать
с помощью функции ``bigrams()``:

    >>> bigrams(['more', 'is', 'said', 'than', 'done'])
    [('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')]
    >>>

Мы видим, что пара слов `than-done`:lx: является биграммой, что на языке
Python записывается в виде ``('than', 'done')``.  Можно сказать, что коллокации |mdash|
это просто часто встречающиеся биграммы (кроме того, нужно обратить особое внимание на
биграммы, которые содержат редкие слова). Так, в частности, нам нужно найти
биграммы, которые встречаются чаще, чем можно было бы предположить исходя лишь
из частоты отдельных слов. Для этого служит
функция ``collocations()`` (и ниже мы увидим, как она работает):

    >>> text4.collocations()
    Building collocations list
    United States; fellow citizens; years ago; Federal Government; General
    Government; American people; Vice President; Almighty God; Fellow
    citizens; Chief Magistrate; Chief Justice; God bless; Indian tribes;
    public debt; foreign nations; political parties; State governments;
    National Government; United Nations; public money
    >>> text8.collocations()
    Building collocations list
    medium build; social drinker; quiet nights; long term; age open;
    financially secure; fun times; similar interests; Age open; poss
    rship; single mum; permanent relationship; slim build; seeks lady;
    Late 30s; Photo pls; Vibrant personality; European background; ASIAN
    LADY; country drives
    >>>

Полученные коллокации сильно зависят от жанра
текста. Чтобы определить, что `red wine`:lx: является коллокацией, нам пришлось
бы обработать гораздо больший массив текстов.

Подсчет других элементов
------------------------

Подсчитывать слова очень полезно, но можно подсчитывать и другие элементы. Например, мы можем
посмотреть на распределение длин слов в тексте, применив функцию ``FreqDist``
к длинному списку чисел, где каждое число представляет собой длину
соответствующего слова в тексте: 

    >>> [len(w) for w in text1] # [_word-lengths]
    [1, 4, 4, 2, 6, 8, 4, 1, 9, 1, 1, 8, 2, 1, 4, 11, 5, 2, 1, 7, 6, 1, 3, 4, 5, 2, ...]
    >>> fdist = FreqDist([len(w) for w in text1])  # [_freq-word-lengths]
    >>> fdist  # [_freq-word-lengths-size]
    <FreqDist with 260819 outcomes>
    >>> fdist.keys()
    [3, 1, 4, 2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20]
    >>>

Сначала мы строим список длин слов в ``text1``
word-lengths_,
а затем с помощью функции ``FreqDist`` подсчитываем число вхождений каждого из этих
значений freq-word-lengths_. Результатом станет freq-word-lengths-size_ распределение, содержащее
примерно 250 тысяч элементов, каждый из которых является числом, соответствующим
слову в тексте. Однако в этом распределении лишь 20 типов
элементов, которые нужно подсчитать. Это числа в диапазоне от 1 до 20, поскольку есть лишь 20
различных длин слов. Таким образом, в тексте есть слова, состоящие из одной буквы,
двух букв и далее до двадцати слов, но ни одного слова из двадцати одной или более
букв. Мы можем задаться вопросом о частоте длин слов
(например, сколько в тексте слов из четырех букв, слов какой длины больше |mdash| из четырех букв или из пяти и т. д.).
Для этого напишем следующий код:

    >>> fdist.items()
    [(3, 50223), (1, 47933), (4, 42345), (2, 38513), (5, 26597), (6, 17111), (7, 14399),
    (8, 9966), (9, 6428), (10, 3528), (11, 1873), (12, 1053), (13, 567), (14, 177),
    (15, 70), (16, 22), (17, 12), (18, 1), (20, 1)]
    >>> fdist.max()
    3
    >>> fdist[3]
    50223
    >>> fdist.freq(3)
    0.19255882431878046
    >>>

Итак, мы видим, что самая частая длина |mdash| 3 буквы, и что в тексте
примерно 50000 трехбуквенных слов (или 20% от всего объема
книги). Хотя на данном этапе нас это не интересует, но дальнейший
анализ длин слов может помочь при выявлении различий между авторами, жанрами
или языками.

В tab-freqdist_ собраны функции, определенные для частотных распределений.

.. table:: tab-freqdist

   ===============================  ======================================================================
   Пример                           Описание
   ===============================  ======================================================================
   ``fdist = FreqDist(samples)``    создание частотного распределения по заданной выборке элементов
   ``fdist.inc(sample)``            увеличение числа вхождений для данного элемента на единицу
   ``fdist['monstrous']``           подсчет числа вхождений заданного значения
   ``fdist.freq('monstrous')``      частота заданного значения
   ``fdist.N()``                    общее число элементов
   ``fdist.keys()``                 элементы, отсортированные по уменьшению частоты
   ``for sample in fdist:``         перебор элементов в порядке увеличения частоты
   ``fdist.max()``                  элемент с самым большим числом вхождений
   ``fdist.tabulate()``             вывод частотного распределения в табличном формате
   ``fdist.plot()``                 построение графика частотного распределения
   ``fdist.plot(cumulative=True)``  построение накопительного графика частотного распределения
   ``fdist1 < fdist2``              проверка того, что элементы в ``fdist1`` встречается реже, чем в ``fdist2``
   ===============================  ======================================================================

   Функции, определенные для частотных распределений |NLTK|

При рассмотрении частотных распределений мы познакомились с некоторыми важными элементами языка Python,
и в sec-making-decisions_ мы изучим их более подробно.

.. 
   We've also touched on the topic of normalization, and we'll explore this in
   depth in chap-words_.   

.. _sec-making-decisions:

----------------------------------------------------------
Подробнее о Python: принятие решений и передача управления
----------------------------------------------------------

.. reimport

   >>> from nltk.book import *

Итак, наше маленькие программы уже обладают некоторыми важными особенностями:
они могут работать с языком и экономить
время пользователей за счет автоматизации рутинных процедур.
Одна из ключевых возможностей программирования заключается в том, что
компьютер может принимать решения за людей, выполняя 
инструкции только при соблюдении определенных условий или многократно
повторяя операции над текстом, пока выполняется некоторое условие. Эта возможность
называется `передачей управления`:dt: и рассматривается в данном разделе.

Условные операторы
------------------

Язык Python поддерживает широкий набор операторов для проверки соотношения
между значениями, например операторы ``<`` и ``>=``. Полный список `операторов
отношения`:dt: показан в tab-inequalities_.

.. table:: tab-inequalities

   ======== =====================================================================================
   Оператор Отношение
   ======== =====================================================================================
   ``<``    меньше
   ``<=``   меньше или равно
   ``==``   равно (обратите внимание, что оператор состоит из двух знаков "``=``", а не из одного
   ``!=``   не равно
   ``>``    больше
   ``>=``   больше или равно
   ======== =====================================================================================

   Операторы сравнения числовых значений

С помощью этих операторов можно выбирать различные слова из предложений, например в текстах новостей.
Ниже показано несколько примеров; они различаются только операторами.
Во всех примерах используется предложение ``sent7`` (первое предложение текста ``text7``
*Wall Street Journal*). Как мы уже говорили, если появится сообщение о том, что переменная ``sent7``
не определена, введите команду ``from nltk.book import *``

.. SB: после инструкций импорта точка не ставится

    >>> sent7
    ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the',
    'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']
    >>> [w for w in sent7 if len(w) < 4]
    [',', '61', 'old', ',', 'the', 'as', 'a', '29', '.']
    >>> [w for w in sent7 if len(w) <= 4]
    [',', '61', 'old', ',', 'will', 'join', 'the', 'as', 'a', 'Nov.', '29', '.']
    >>> [w for w in sent7 if len(w) == 4]
    ['will', 'join', 'Nov.']
    >>> [w for w in sent7 if len(w) != 4]
    ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'the', 'board',
    'as', 'a', 'nonexecutive', 'director', '29', '.']
    >>> 

Во всех этих примерах используется один и тот же шаблон
``[w for w in text if`` *условие* ``]``, где *условие* |mdash|
это проверка, результатом которой является значение "истина" или "ложь".
В показанных примерах в роли условия всегда выступает сравнение чисел.
Но мы можем проверять и различные свойства слов; для этого служат функции,
перечисленные в tab-word-tests_.

.. table:: tab-word-tests

   ====================   =======================================================================================
   Функция                Значение
   ====================   =======================================================================================
   ``s.startswith(t)``    проверка того, что ``s`` начинается на ``t``
   ``s.endswith(t)``      проверка того, что ``s`` оканчивается на ``t``
   ``t in s``             проверка того, что ``t`` содержится в ``s``
   ``s.islower()``        проверка того, что ``s`` состоит только из строчных букв
   ``s.isupper()``        проверка того, что ``s`` состоит только из прописных букв
   ``s.isalpha()``        проверка того, что ``s`` состоит только из букв
   ``s.isalnum()``        проверка того, что ``s`` состоит только из букв и цифр
   ``s.isdigit()``        проверка того, что ``s`` состоит только из цифр
   ``s.istitle()``        проверка того, что ``s`` имеет формат заголовка (все слова начинаются с прописной буквы)
   ====================   ========================================================================================

   Некоторые операторы сравнения, применяемые к словам

Ниже показано несколько примеров использования этих
операторов для выбора из текста следующих категорий слов:
слова, оканчивающиеся на `-ableness`:lx:;
слова, содержащие `gnt`:lx:;
слова, начинающиеся с прописной буквы;
слова, состоящие только из цифр.

    >>> sorted([w for w in set(text1) if w.endswith('ableness')])
    ['comfortableness', 'honourableness', 'immutableness', 'indispensableness', ...]
    >>> sorted([term for term in set(text4) if 'gnt' in term])
    ['Sovereignty', 'sovereignties', 'sovereignty']
    >>> sorted([item for item in set(text6) if item.istitle()])
    ['A', 'Aaaaaaaaah', 'Aaaaaaaah', 'Aaaaaah', 'Aaaah', 'Aaaaugh', 'Aaagh', ...] 
    >>> sorted([item for item in set(sent7) if item.isdigit()])
    ['29', '61']
    >>>

Мы также можем создавать более сложные условия. Если `c`:math: является
условием, то ``not`` `c`:math: также является условием.
Если у нас есть условия `c`:math:\ `1`:subscript: и `c`:math:\ `2`:subscript:,
то мы можем объединить их в новое условие с помощью конъюнкции или дизъюнкции:
`c`:math:\ `1`:subscript: ``and`` `c`:math:\ `2`:subscript:,
`c`:math:\ `1`:subscript: ``or`` `c`:math:\ `2`:subscript:.

.. note:: |TRY|
   Запустите следующие примеры и попробуйте объяснить, как работает каждый из них.
   После этого попробуйте самостоятельно придумать какие-либо условия.

   .. doctest-ignore::
      >>> sorted([w for w in set(text7) if '-' in w and 'index' in w])
      >>> sorted([wd for wd in set(text3) if wd.istitle() and len(wd) > 10])
      >>> sorted([w for w in set(sent7) if not w.islower()])
      >>> sorted([t for t in set(text2) if 'cie' in t or 'cei' in t])

Выполнение операций над каждым элементом
----------------------------------------

В sec-computing-with-language-simple-statistics_ мы видели несколько примеров
подсчета элементов, отличных от слов. Рассмотрим используемую при этом нотацию подробнее:

    >>> [len(w) for w in text1]
    [1, 4, 4, 2, 6, 8, 4, 1, 9, 1, 1, 8, 2, 1, 4, 11, 5, 2, 1, 7, 6, 1, 3, 4, 5, 2, ...]
    >>> [w.upper() for w in text1]
    ['[', 'MOBY', 'DICK', 'BY', 'HERMAN', 'MELVILLE', '1851', ']', 'ETYMOLOGY', '.', ...]
    >>>

Эти выражения имеют форму ``[f(w) for ...]`` или ``[w.f() for ...]``, где
``f`` |mdash| это функция, применяемая к слову для вычисления его длины
или преобразования всех его букв в прописные.
На данном этапе вам не обязательно понимать разницу между обозначениями ``f(w)`` и
``w.f()``. Просто запомните эту конструкцию Python, которая служит
для выполнения одной и той же операции над каждым из элементов списка. В вышеприведенных примерах перебираются
все слова в ``text1``, каждое из них присваивается переменной ``w``, а затем
над переменной выполняется указанная операция.

.. note::
   Описанная нотация называется "свертыванием списков". Это наш первый пример
   "идиомы" Python |mdash| устойчивой конструкции, которую мы используем регулярно
   и без детального анализа каждого элемента. Свободное владение такими идиомами является
   важным отличием хороших программистов на Python.

Вернемся к вопросу о размере словаря и воспользуемся этой же конструкцией:

    >>> len(text1)
    260819
    >>> len(set(text1))
    19317
    >>> len(set([word.lower() for word in text1]))
    17231
    >>>

Мы больше не учитываем слова, различающиеся только регистром (например, `This`:lx: и `this`:lx:) дважды,
в результате чего размер словаря уменьшился на 2000 слов! Пойдем еще дальше и очистим
словарь от знаков препинания и чисел, исключив элементы, не состоящие
только из букв:

    >>> len(set([word.lower() for word in text1 if word.isalpha()]))
    16948
    >>> 

Этот пример немного сложнее: в нижний регистр переводятся все элементы, состоящие только из букв.
Возможно, было бы проще подсчитывать только элементы в нижнем регистре, но
результат при этом будет неверным (почему?).

Не переживайте, если свертывание списков пока представляется вам не очень понятным,
в последующих главах вы встретите еще много примеров с объяснениями.

Вложенные блоки кода
--------------------

Большинство языков программирования позволяют выполнять блок кода, если
выполняется `условное выражение`:dt: или условие ``if``. Мы
уже видели примеры проверки условий в коде вида ``[w for w in
sent7 if len(w) < 4]``. В следующей программе мы создаем
переменную с именем ``word``, содержащую строковое значение ``'cat'``.
С помощью оператора ``if`` мы проверяем истинность выражения ``len(word) < 5``.
Если выражение истинно, вызывается тело оператора ``if`` и выполняется
инструкция ``print``, которая выводит на экран сообщение.
Не забудьте выделить инструкцию ``print`` отсупом из четырех пробелов.

    >>> word = 'cat'
    >>> if len(word) < 5:
    ...     print 'word length is less than 5'
    ...   # [_blank-line]
    word length is less than 5
    >>>

При работе с интерпретатором Python мы вставляем дополнительную пустую строку blank-line_,
чтобы обозначить конец вложенного блока.

Если мы изменим условие на ``len(word) >= 5`` 
(длина слова ``word`` больше или равна ``5``),
то результатом проверки больше не будет значение "истина".
В этом случае тело оператора ``if`` выполняться не будет,
а пользователь не увидит сообщения:

    >>> if len(word) >= 5:
    ...   print 'word length is greater than or equal to 5'
    ... 
    >>>

Оператор ``if`` также называют `управляющей структурой`:dt:,
поскольку он определяет, будет ли выполняться код в выделенном отступом блоке.
Еще одной управляющей структурой является цикл ``for``.
Попробуйте выполнить следующий код, не забудьте добавить двоеточие и четыре пробела:

    >>> for word in ['Call', 'me', 'Ishmael', '.']:
    ...     print word
    ...
    Call
    me
    Ishmael
    .
    >>>

Такая конструкция называется циклом, поскольку код
выполняется многократно. Программа начинается с
присваивания ``word = 'Call'``;
при этом переменная ``word`` используется для именования
первого элемента списка. Затем значение ``word``
выводится на экран. После этого программа возвращается к оператору ``for`` и
выполняет присваивание ``word = 'me'``, чтобы затем вывести на экран 
это новое значение, и так далее. Эти операции повторяются до тех пор,
пока не будут обработаны все элементы списка.

Циклы с условиями
-----------------

Теперь мы можем объединить операторы ``if`` и ``for``.
Выполним циклический перебор всех элементов списка и напечатаем
только те из них, которые оканчиваются на букву *l*. Выберем другое имя переменной,
чтобы подчеркнуть, что с точки зрения интерпретатора Python
имена переменных не несут никакого смысла.

    >>> sent1 = ['Call', 'me', 'Ishmael', '.']
    >>> for xyzzy in sent1:
    ...     if xyzzy.endswith('l'):
    ...         print xyzzy
    ...
    Call
    Ishmael
    >>>

Можно заметить, что в конце строк с операторами
``if`` и ``for`` и перед блоком с отступами
ставится двоеточие. На самом деле все управляющие структуры Python
оканчиваются двоеточием. Двоеточие указывает,
что текущий оператор относится к последующему выделенному отступами
блоку.

Мы также можем задать действие, которое будет выполнено,
если условие в операторе ``if`` не выполняется.
В следующем примере используются операторы ``elif`` (else if) и
``else``. Обратите внимание, что здесь
перед выделенным отступом кодом также стоят двоеточия.

    >>> for token in sent1:
    ...     if token.islower():
    ...         print token, 'is a lowercase word'
    ...     elif token.istitle():
    ...         print token, 'is a titlecase word'
    ...     else:
    ...         print token, 'is punctuation'
    ...
    Call is a titlecase word
    me is a lowercase word
    Ishmael is a titlecase word
    . is punctuation
    >>>

Мы видим, что знание даже небольшого числа возможностей Python
позволяет нам писать программы, состоящие из нескольких строк.
Очень важно разрабатывать программы именно такими фрагментами
и проверять, что каждый фрагмент работает надлежащим образом, прежде
чем объединять фрагменты в программу. Именно это делает интерактивный интерпретатор Python
таким ценным инструментом, и именно поэтому вам нужно научиться
уверенно пользоваться им.

В завершении мы объединим рассмотренные конструкции.
Сначала мы создадим список слов с `cie`:lx: и `cei`:lx:,
а затем переберем его, чтобы вывести каждый элемент на экран. Обратите внимание на запятую
после оператора print, которая указывает на то,
что все элементы должны выводиться на одной строке.

    >>> tricky = sorted([w for w in set(text2) if 'cie' in w or 'cei' in w])
    >>> for word in tricky:
    ...     print word,
    ancient ceiling conceit conceited conceive conscience
    conscientious conscientiously deceitful deceive ...
    >>>

.. _sec-automatic-natural-language-understanding:

----------------------------------------
Automatic Natural Language Understanding
----------------------------------------

..
    >>> from nltk.misc import babelize_shell

We have been exploring language bottom-up, with the help of texts and
the Python programming
language.  However, we're also interested in exploiting our knowledge of language and computation
by building useful language technologies. We'll take the opportunity
now to step back from the nitty-gritty of code in order to paint a
bigger picture of natural language processing.

At a purely practical level, we all need help to navigate the universe of information
locked up in text on the Web.  Search engines have been crucial to the
growth and popularity of the Web, but have some shortcomings.
It takes skill, knowledge, and some luck,
to extract answers to such questions as: `What tourist sites can I
visit between Philadelphia and Pittsburgh on a limited budget?`:lx:
`What do experts say about digital SLR cameras?`:lx: `What
predictions about the steel market were made by credible commentators
in the past week?`:lx: Getting a computer to answer them automatically
involves a range of language processing tasks, including information extraction,
inference, and summarization, and would need to be carried out on a scale
and with a level of robustness that is still beyond our current capabilities.

On a more philosophical level, a long-standing challenge within artificial intelligence
has been to build intelligent machines, and a major part of intelligent behaviour is understanding
language.  For many years this goal has been seen as too difficult.
However, as |NLP| technologies become more mature, and robust methods for
analyzing unrestricted text become more widespread, the prospect of
natural language understanding has re-emerged as a plausible goal.

In this section we describe some language understanding technologies,
to give you a sense of the interesting challenges that are waiting for you.

Word Sense Disambiguation
-------------------------

In `word sense disambiguation`:dt: we want to work out
which sense of a word was intended in a given context.  Consider the
ambiguous words `serve`:lx: and `dish`:lx:\ :

.. ex::
    .. ex:: `serve`:lx:\ : help with food or drink; hold an office; put ball into play
    .. ex:: `dish`:lx:\ : plate; course of a meal; communications device

In a sentence containing the phrase: `he served the dish`:lx:, you
can detect that both `serve`:lx: and `dish`:lx: are being used with
their food meanings.  It's unlikely that the topic of discussion
shifted from sports to crockery in the space of three words.
This would force you to invent bizarre images, like a tennis pro
taking out their frustrations on a china tea-set laid out beside the court. 
In other words, we automatically disambiguate words using context, exploiting
the simple fact that nearby words have closely related meanings.
As another example of this contextual effect, consider the word
`by`:lx:, which has several meanings, e.g.: `the book by
Chesterton`:lx: (agentive |mdash| Chesterton was the author of the book);
`the cup by the stove`:lx: (locative |mdash| the stove is where the
cup is); and `submit by Friday`:lx: (temporal |mdash| Friday is the
time of the submitting).
Observe in ex-lost-children_ that the meaning of the italicized word helps us
interpret the meaning of `by`:lx:.

.. _ex-lost-children:
.. ex::
   .. ex:: The lost children were found by the `searchers`:em:  (agentive)
   .. ex:: The lost children were found by the `mountain`:em:   (locative)
   .. ex:: The lost children were found by the `afternoon`:em:  (temporal)

Pronoun Resolution
------------------

A deeper kind of language understanding is to work out "who did what to whom" |mdash|
i.e., to detect the subjects and objects of verbs.  You learnt to do this in
elementary school, but it's harder than you might think.
In the sentence `the thieves stole the paintings`:lx:
it is easy to tell who performed the stealing action.
Consider three possible following sentences in ex-thieves_, and try to determine
what was sold, caught, and found (one case is ambiguous).
  
.. _ex-thieves:
.. ex::
   .. ex:: The thieves stole the paintings.  They were subsequently `sold`:em:.
   .. ex:: The thieves stole the paintings.  They were subsequently `caught`:em:.
   .. ex:: The thieves stole the paintings.  They were subsequently `found`:em:.

Answering this question involves finding the `antecedent`:dt: of the pronoun `they`:lx:,
either thieves or paintings.  Computational techniques for tackling this problem
include `anaphora resolution`:dt: |mdash| identifying what a pronoun or noun phrase
refers to |mdash| and `semantic role labeling`:dt: |mdash| identifying how a noun phrase
relates to the verb (as agent, patient, instrument, and so on).

Generating Language Output
--------------------------

If we can automatically solve such problems of language understanding, we will
be able to move on to tasks that involve generating language output, such as
`question answering`:dt: and `machine translation`:dt:.  In the first case,
a machine should be able to answer a user's questions relating to collection of texts:

.. _ex-qa-application:
.. ex::
   .. ex:: *Text:* ... The thieves stole the paintings.  They were subsequently sold. ...
   .. ex:: *Human:* Who or what was sold?
   .. ex:: *Machine:* The paintings.

The machine's answer demonstrates that it has correctly worked out that `they`:lx:
refers to paintings and not to thieves.  In the second case, the machine should
be able to translate the text into another language, accurately
conveying the meaning of the original text.  In translating the example text into French,
we are forced to choose the gender of the pronoun in the second sentence:
`ils`:lx: (masculine) if the thieves are found, and `elles`:lx: (feminine) if
the paintings are found.  Correct translation actually depends on correct understanding of
the pronoun.

.. _ex-mt-application:
.. ex::
   .. ex:: The thieves stole the paintings.  They were subsequently found.
   .. ex:: Les voleurs ont vol\ |eacute| les peintures. Ils ont |eacute|\ t\ |eacute| trouv\ |eacute|\ s plus tard.  (the thieves)
   .. ex:: Les voleurs ont vol\ |eacute| les peintures. Elles ont |eacute|\ t\ |eacute| trouv\ |eacute|\ es plus tard.  (the paintings)
    
In all of these examples, working out the sense of a word, the subject of a verb, and the 
antecedent of a pronoun are steps in establishing the meaning of a sentence, things
we would expect a language understanding system to be able to do.

Machine Translation
-------------------

For a long time now, machine translation (MT) has
been the holy grail of language understanding,
ultimately seeking to provide high-quality,
idiomatic translation between any pair of languages.
Its roots go back to the early days of the Cold War, when the promise
of automatic translation led to substantial government sponsorship,
and with it, the genesis of |NLP| itself.

Today, practical translation systems exist for particular pairs
of languages, and some are integrated into web search engines.
However, these systems have some serious shortcomings.
We can explore them with the help of |NLTK|\ 's "babelizer"
(which is automatically loaded when you import ths chapter's
materials using ``from nltk.book import *``).
This program submits a sentence for translation into a
specified language, then submits the resulting sentence for translation
back into English.  It stops after 12 iterations, or if it receives
a translation that was produced already (indicating a loop):

.. doctest-ignore::
    >>> babelize_shell()
    NLTK Babelizer: type 'help' for a list of commands.
    Babel> how long before the next flight to Alice Springs?
    Babel> german
    Babel> run
    0> how long before the next flight to Alice Springs?
    1> wie lang vor dem folgenden Flug zu Alice Springs?
    2> how long before the following flight to Alice jump?
    3> wie lang vor dem folgenden Flug zu Alice springen Sie?
    4> how long before the following flight to Alice do you jump?
    5> wie lang, bevor der folgende Flug zu Alice tun, Sie springen?
    6> how long, before the following flight to Alice does, do you jump?
    7> wie lang bevor der folgende Flug zu Alice tut, tun Sie springen?
    8> how long before the following flight to Alice does, do you jump?
    9> wie lang, bevor der folgende Flug zu Alice tut, tun Sie springen?
    10> how long, before the following flight does to Alice, do do you jump?
    11> wie lang bevor der folgende Flug zu Alice tut, Sie tun Sprung?
    12> how long before the following flight does leap to Alice, does you?

Observe that the system correctly translates `Alice Springs`:lx: from English
to German (in the line starting ``1>``), but on the way back to English, this ends up as `Alice jump`:lx:
(line ``2``).  The preposition `before`:lx: is initially translated into the corresponding
German preposition `vor`:lx:, but later into the conjunction `bevor`:lx: (line 5).
After line ``5`` the sentences become nonsensical (but notice the various phrasings
indicated by the commas, and the change from `jump`:lx: to `leap`:lx:).
The translation system did not recognize when a word was part of a proper name,
and it misinterpreted the grammatical structure.  The grammatical problems
are more obvious in the following example.  Did John find the pig, or did the
pig find John?

.. doctest-ignore::
    >>> babelize_shell()
    Babel> The pig that John found looked happy
    Babel> german 
    Babel> run
    0> The pig that John found looked happy
    1> Das Schwein, das John fand, schaute gl?cklich
    2> The pig, which found John, looked happy

Machine translation is difficult because a given word could have several possible
translations (depending on its meaning), and because word order must be changed
in keeping with the grammatical structure of the target language.
Today these difficulties are being faced by collecting massive quantities of
parallel texts from news and government websites that publish documents
in two or more languages.  Given a document in German and English, and possibly
a bilingual dictionary, we can automatically pair up the sentences,
a process called `text alignment`:dt:.  Once we have a million or more sentence
pairs, we can detect corresponding words and phrases, and build a model
that can be used for translating new text.

Spoken Dialog Systems
---------------------

In the history of artificial intelligence, the chief measure of intelligence
has been a linguistic one, namely the `Turing Test`:dt:\ : can a dialogue system,
responding to a user's text input, perform so naturally that we cannot distinguish
it from a human-generated response?  In contrast, today's commercial dialogue systems
are very limited, but still perform useful functions in narrowly-defined domains,
as we see here:

| S: How may I help you?
| U: When is Saving Private Ryan playing?
| S: For what theater?
| U: The Paramount theater.
| S: Saving Private Ryan is not playing at the Paramount theater, but
| it's playing at the Madison theater at 3:00, 5:30, 8:00, and 10:30. 

You could not ask this system to provide driving instructions or
details of nearby restaurants unless the required information
had already been stored and suitable question-answer pairs
had been incorporated into the language processing system.

Observe that this system seems to understand the user's goals:
the user asks when a movie is showing and the system
correctly determines from this that the user wants to see
the movie. This inference seems so obvious that you probably
didn't notice it was made, yet a natural language system
needs to be endowed with this capability in order to interact
naturally.  Without it, when asked `Do you know when Saving Private
Ryan is playing?`:lx:, a system might unhelpfully respond with a cold `Yes`:lx:.
However, the developers of commercial dialogue systems use
contextual assumptions and business logic to ensure that the different ways in which a user might
express requests or provide information are handled in a way that
makes sense for the particular application.  So, if you type
`When is ...`:lx:, or `I want to know when ...`:lx:, or `Can you tell me
when ...`:lx:, simple rules will always yield screening times.  This is
enough for the system to provide a useful service.

.. _fig-sds:
.. figure:: ../images/dialogue.png
   :scale: 25:32:30

   Simple Pipeline Architecture for a Spoken Dialogue System:
   Spoken input (top left) is analyzed, words are recognized, sentences are parsed and
   interpreted in context, application-specific actions take place (top right);
   a response is planned, realized as a syntactic structure, then to suitably
   inflected words, and finally to spoken output; different types of
   linguistic knowledge inform each stage of the process.

Dialogue systems give us an opportunity to mention the
commonly assumed pipeline for |NLP|.
fig-sds_ shows the architecture of a simple dialogue system.
Along the top of the diagram, moving from left to right, is a
"pipeline" of some language understanding `components`:dt:.
These map from speech input via syntactic parsing
to some kind of meaning representation.  Along the middle, moving from
right to left, is the reverse pipeline of components for converting
concepts to speech.  These components make up the dynamic aspects of the system.
At the bottom of the diagram are some representative bodies of
static information: the repositories of language-related data that
the processing components draw on to do their work.

.. note:: |TRY|
   For an example of a primitive dialogue system, try having
   a conversation with an NLTK chatbot.  To see the available chatbots,
   run ``nltk.chat.chatbots()``.
   (Remember to ``import nltk`` first.) 

Textual Entailment
------------------

The challenge of language understanding has been brought into focus in recent years by a public
"shared task" called Recognizing Textual Entailment (RTE). The basic
scenario is simple.  Suppose you want to find evidence to support
the hypothesis: `Sandra Goudie was defeated by Max Purnell`:lx:, and
that you have another short text that seems to be relevant, for example,
`Sandra Goudie was first elected to Parliament in the 2002 elections,
narrowly winning the seat of Coromandel by defeating Labour candidate
Max Purnell and pushing incumbent Green MP Jeanette Fitzsimons into
third place`:lx:.  Does the text provide enough evidence for you to
accept the hypothesis?  In this particular case, the answer will be "No."
You can draw this conclusion easily, but it is very hard to come up with 
automated methods for making the right decision. The RTE
Challenges provide data that allow competitors to develop their
systems, but not enough data for "brute force" machine learning techniques (a topic
we will cover in chap-data-intensive_\ ).  Consequently, some
linguistic analysis is crucial. In the previous example, it is important
for the system to note that `Sandra Goudie`:lx: names the person being
defeated in the hypothesis, not the person doing the defeating in the
text. As another illustration of the difficulty of the task, consider
the following text-hypothesis pair:

.. ex::
   .. ex:: Text: David Golinkin is the editor or author of eighteen books, and over 150 responsa, articles, sermons and books 
   .. ex:: Hypothesis: Golinkin has written eighteen books

In order to determine whether the hypothesis is supported by the
text, the system needs the following background knowledge:
(i) if someone is an author of a book, then he/she has written that
book; (ii) if someone is an editor of a book, then he/she has not
written (all of) that book; (iii) if someone is editor or author of eighteen
books, then one cannot conclude that he/she is author of eighteen books. 

Limitations of |NLP|
--------------------

Despite the research-led advances in tasks like RTE, natural language
systems that have been deployed for real-world applications still cannot perform
common-sense reasoning or draw on world knowledge in a general and
robust manner.  We can wait for these difficult artificial
intelligence problems to be solved, but in the meantime it is
necessary to live with some severe limitations on the reasoning and
knowledge capabilities of natural language systems. Accordingly, right
from the beginning, an important goal of |NLP| research has been to
make progress on the difficult task of building technologies that
"understand language," using superficial yet powerful techniques instead of
unrestricted knowledge and reasoning capabilities.
Indeed, this is one of the goals of this book, and we hope to equip you with
the knowledge and skills to build useful |NLP| systems, and to
contribute to the long-term aspiration of building intelligent machines.

-------
Summary
-------

* Texts are represented in Python using lists:
  ``['Monty', 'Python']``.  We can use indexing, slicing,
  and the ``len()`` function on lists.
* A word "token" is a particular appearance of a given word in a text;
  a word "type" is the unique form of the word as a particular sequence
  of letters.  We count word tokens using ``len(text)`` and word types using
  ``len(set(text))``. 
* We obtain the vocabulary of a text ``t`` using ``sorted(set(t))``.
* We operate on each item of a text using ``[f(x) for x in text]``. 
* To derive the vocabulary, collapsing case distinctions and ignoring punctuation,
  we can write ``set([w.lower() for w in text if w.isalpha()])``. 
* We process each word in a text using a ``for`` statement, such
  as ``for w in t:`` or ``for word in text:``.  This must be followed by the colon character
  and an indented block of code, to be executed each time through the loop.
* We test a condition using an ``if`` statement: ``if len(word) < 5:``.
  This must be followed by the colon character and an indented block of
  code, to be executed only if the condition is true.
* A frequency distribution is a collection of items along with their frequency counts
  (e.g., the words of a text and their frequency of appearance).
* A function is a block of code that has been assigned a name and can
  be reused. Functions are defined using the ``def`` keyword, as in
  ``def mult(x, y)``; ``x`` and ``y`` are parameters of the function,
  and act as placeholders for actual data values.
* A function is called by specifying its name followed by one or more
  arguments inside parentheses, like this: ``mult(3, 4)``, e.g., ``len(text1)``.

---------------
Further Reading
---------------

This chapter has introduced new concepts in programming, natural language processing,
and linguistics, all mixed in together.
Many of them are consolidated in the following chapters.  However, you may also want to
consult the online materials provided with this chapter (at |NLTK-URL|), including links
to additional background materials, and links to online |NLP| systems.
You may also like to read up on
some linguistics and |NLP|\ -related concepts in Wikipedia (e.g., collocations,
the Turing Test, the type-token distinction).

You should acquaint yourself with the Python documentation available at |PYTHON-DOCS|,
including the many tutorials and comprehensive reference materials linked there.
A `Beginner's Guide to Python`:em: is available at ``http://wiki.python.org/moin/BeginnersGuide``.
Miscellaneous questions about Python might be answered in the FAQ at
``http://www.python.org/doc/faq/general/``.

As you delve into |NLTK|, you might want to subscribe to the mailing list where new
releases of the toolkit are announced.  There is also an NLTK-Users mailing list,
where users help each other as they learn how to use Python and |NLTK| for
language analysis work.  Details of these lists are available at |NLTK-URL|.

For more information on the topics covered in sec-automatic-natural-language-understanding_,
and on |NLP| more generally, you might like to consult one of the following excellent
books:

* Indurkhya, Nitin and Fred Damerau (eds, 2010) *Handbook of Natural Language Processing*
  (Second Edition) Chapman & Hall/CRC. 2010.  [IndurkhyaDamerau2010]_ [Dale00handbook]_

* Jurafsky, Daniel and James Martin (2008) *Speech and Language Processing* (Second Edition).  Prentice Hall.
  [JurafskyMartin2008]_

* Mitkov, Ruslan (ed, 2003) *The Oxford Handbook of Computational Linguistics*.  Oxford University Press.
  (second edition expected in 2010).  [Mitkov02handbook]_

The Association for Computational Linguistics is the international organization that
represents the field of |NLP|.  The ACL website (``http://www.aclweb.org/``) hosts many useful resources, including:
information about international and regional conferences and workshops;
the `ACL Wiki`:em: with links to hundreds of useful resources;
and the `ACL Anthology`:em:, which contains most of the |NLP| research literature
from the past 50 years, fully indexed and freely downloadable.

Some excellent introductory Linguistics textbooks are:
[Finegan2007]_, [OGrady2004]_, [OSU2007]_.  You might like to consult
`LanguageLog`:em:, a popular linguistics blog with occasional posts that
use the techniques described in this book.

---------
Exercises
---------

.. TODO: add lots more exercises on lists!

#. |easy| Try using the Python interpreter as a calculator, and
   typing expressions like ``12 / (4 + 1)``.
   
#. |easy| Given an alphabet of 26 letters, there are 26 to the power
   10, or ``26 ** 10``, ten-letter strings we can form.  That works out
   to ``141167095653376L`` (the ``L`` at the end just indicates that
   this is Python's long-number format).  How many hundred-letter
   strings are possible?

#. |easy| The Python multiplication operation can be applied to lists.
   What happens when you type ``['Monty', 'Python'] * 20``,
   or ``3 * sent1``?

#. |easy| Review sec-computing-with-language-texts-and-words_ on
   computing with language.  How many words are there in ``text2``?
   How many distinct words are there?

#. |easy| Compare the lexical diversity scores for humor
   and romance fiction in tab-brown-types_.  Which genre is
   more lexically diverse?

#. |easy| Produce a dispersion plot of the four main protagonists in
   *Sense and Sensibility*: Elinor, Marianne, Edward, and Willoughby.
   What can you observe about the different roles played by the males
   and females in this novel?  Can you identify the couples?

#. |easy| Find the collocations in ``text5``.

#. |easy| Consider the following Python expression: ``len(set(text4))``.
   State the purpose of this expression.  Describe the two steps
   involved in performing this computation.

#. |easy| Review sec-a-closer-look-at-python-texts-as-lists-of-words_
   on lists and strings.
   
   a) Define a string and assign it to a variable, e.g.,
      ``my_string = 'My String'`` (but put something more interesting in the string).
      Print the contents of this variable in two ways, first
      by simply typing the variable name and pressing enter, then
      by using the ``print`` statement.
   
   b) Try adding the string to itself using ``my_string + my_string``, or multiplying
      it by a number, e.g., ``my_string * 3``.  Notice that the strings
      are joined together without any spaces.  How could you fix this?

#. |easy| Define a variable ``my_sent`` to be a list of words, using
   the syntax ``my_sent = ["My", "sent"]`` (but with your own words,
   or a favorite saying).
   
   a) Use ``' '.join(my_sent)`` to convert this into a string.
   b) Use ``split()`` to split the string back into the list form
      you had to start with.

#. |easy| Define several variables containing lists of words, e.g., ``phrase1``,
   ``phrase2``, and so on.  Join them together in various combinations (using the plus operator)
   to form whole sentences.  What is the relationship between
   ``len(phrase1 + phrase2)`` and ``len(phrase1) + len(phrase2)``?

#. |easy| Consider the following two expressions, which have the same
   value.  Which one will typically be more relevant in |NLP|?  Why?

   a) ``"Monty Python"[6:12]``
   b) ``["Monty", "Python"][1]``

#. |easy| We have seen how to represent a sentence as a list of words, where
   each word is a sequence of characters.  What does ``sent1[2][2]`` do?
   Why?  Experiment with other index values.

#. |easy| The first sentence of ``text3`` is provided to you in the
   variable ``sent3``.  The index of `the`:lx: in ``sent3`` is 1, because ``sent3[1]``
   gives us ``'the'``.  What are the indexes of the two other occurrences
   of this word in ``sent3``?

#. |easy| Review the discussion of conditionals in sec-making-decisions_.
   Find all words in the Chat Corpus (``text5``)
   starting with the letter `b`:lx:.  Show them in alphabetical order.

#. |easy| Type the expression ``range(10)`` at the interpreter prompt.
   Now try ``range(10, 20)``, ``range(10, 20, 2)``, and ``range(20, 10, -2)``.
   We will see a variety of uses for this built-in function in later chapters.

#. |soso| Use ``text9.index()`` to find the index of the word `sunset`:lx:.
   You'll need to insert this word as an argument between the parentheses.
   By a process of trial and error, find the slice for the complete sentence that
   contains this word.

#. |soso| Using list addition, and the ``set`` and ``sorted`` operations, compute the
   vocabulary of the sentences ``sent1`` ... ``sent8``.

#. |soso| What is the difference between the following two lines?
   Which one will give a larger value?  Will this be the case for other texts?
   
   .. doctest-ignore::
       >>> sorted(set([w.lower() for w in text1]))
       >>> sorted([w.lower() for w in set(text1)])  

#. |soso| What is the difference between the following two tests:
   ``w.isupper()`` and ``not w.islower()``?

#. |soso| Write the slice expression that extracts the last two words of ``text2``.

#. |soso| Find all the four-letter words in the Chat Corpus (``text5``).
   With the help of a frequency distribution (``FreqDist``), show these
   words in decreasing order of frequency.

#. |soso| Review the discussion of looping with conditions in sec-making-decisions_.
   Use a combination of ``for`` and ``if`` statements to loop over the words of
   the movie script for *Monty Python and the Holy Grail* (``text6``)
   and ``print`` all the uppercase words, one per line.

#. |soso| Write expressions for finding all words in ``text6`` that
   meet the following conditions.  The result should be in the form of
   a list of words: ``['word1', 'word2', ...]``.
   
   a) Ending in `ize`:lx:
   b) Containing the letter `z`:lx:
   c) Containing the sequence of letters `pt`:lx:
   d) All lowercase letters except for an initial capital (i.e., ``titlecase``)

#. |soso| Define ``sent`` to be the list of words
   ``['she', 'sells', 'sea', 'shells', 'by', 'the', 'sea', 'shore']``.
   Now write code to perform the following tasks:

   a) Print all words beginning with `sh`:lx:
   b) Print all words longer than four characters

#. |soso| What does the following Python code do?  ``sum([len(w) for w in text1])``
   Can you use it to work out the average word length of a text?

#. |soso| Define a function called ``vocab_size(text)`` that has a single
   parameter for the text, and which returns the vocabulary size of the text.

#. |soso| Define a function ``percent(word, text)`` that calculates
   how often a given word occurs in a text, and expresses the result
   as a percentage.

#. |soso| We have been using sets to store vocabularies.  Try the following
   Python expression: ``set(sent3) < set(text1)``.  Experiment with this using
   different arguments to ``set()``.  What does it do?
   Can you think of a practical application for this?

.. include:: footer.rst
